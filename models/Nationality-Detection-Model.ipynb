{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "376e0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "from tensorflow.keras.models import load_model, Model, model_from_json\n",
    "import cv2\n",
    "import tkinter as tk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf5ff3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67608369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dress_color import estimate_dress_colour, nearest_colour_name, dominant_colour_kmean\n",
    "from age_gender_prediction import detect_faces, predict_age, predict_gender,face_net,face_proto,age_net,gender_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775ac7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and compiled successfull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 20 variables whereas the saved optimizer has 38 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "emotion_model = load_model(\"emotion_detector.h5\")\n",
    "\n",
    "with open('model_arch.json') as json_file:\n",
    "    loaded_model= json_file.read()\n",
    "\n",
    "emotion_model = model_from_json(loaded_model)\n",
    "\n",
    "emotion_model.load_weights(\"model.weights.h5\")\n",
    "\n",
    "emotion_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Model loaded and compiled successfull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae86095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and compiled successfully\n"
     ]
    }
   ],
   "source": [
    "nationality_model= load_model(\"nationality_detection.h5\")\n",
    "\n",
    "with open('nationality_arch.json') as json_file:\n",
    "    loaded_model= json_file.read()\n",
    "\n",
    "nationality_model = model_from_json(loaded_model)\n",
    "\n",
    "nationality_model.load_weights(\"nationalitymodel.weights.h5\")\n",
    "\n",
    "nationality_model.compile(loss = \"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Model loaded and compiled successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae9e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMO_INPUT_SIZE = 48\n",
    "EMOTION_LABELS = [\"Angry\",\"Disgust\",\"Fear\",\"Happy\",\"Sad\",\"Surprise\",\"Neutral\"]\n",
    "\n",
    "NAT_INPUT_SIZE = 224\n",
    "NAT_LABELS = [\"Indian\",\"USA\",\"African\",\"Other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97dd5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(face_bgr):\n",
    "    gray = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(gray, (EMO_INPUT_SIZE, EMO_INPUT_SIZE))\n",
    "    img = img.astype(\"float32\")/255.0\n",
    "    img= np.expand_dims(img, axis= -1)\n",
    "    img = np.expand_dims(img , axis= 0)\n",
    "    preds = emotion_model.predict(img, verbose=[0])[0]\n",
    "    idx = int(np.argmax(preds))\n",
    "    return EMOTION_LABELS[idx], float (preds[idx]), preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71ee8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(face_bgr):\n",
    "    img = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (NAT_INPUT_SIZE,NAT_INPUT_SIZE))\n",
    "    img = img.astype(\"float32\")/255.0\n",
    "    img = np.expand_dims(img, 0)\n",
    "    preds = nationality_model.predict(img, verbose= 0)[0]\n",
    "    idx = int(np.argmax(preds))\n",
    "    return NAT_LABELS[idx], float (preds[idx]), preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class App:\n",
    "    def __init__ (self,root):\n",
    "        self.root = root\n",
    "        root.title(\"Nationality , Gender, Age, Dress Color and Emotion detection GUI\" )\n",
    "        root.geometry(\"1000x640\")\n",
    "        self.img_bgr = None\n",
    "        self.imgtk = None\n",
    "        self.canvas = tk.Canvas(root, width = 600, height= 520, bg= \"black\")\n",
    "        self.canvas.place(x=20, y=20)\n",
    "\n",
    "        tk.Button(root, text=\"Open Image\", command=self.open_image).place(x=600, y=30, width=120,  height=40)\n",
    "        tk.Button(root, text= \"Run\", command=self.run_inference).place(x=700, y=30, width=120, height=40)\n",
    "        tk.Button(root, text=\"Clear\", command= self.clear).place(x=820, y=30, width= 60, height= 40)\n",
    "        self.output= tk.Text(root, width= 40, height= 28)\n",
    "        self.output.place(x= 640, y=80)\n",
    "\n",
    "    \n",
    "    def open_image(self):\n",
    "        path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp\"), (\"All files\", \"*.*\")])\n",
    "\n",
    "        if not path:\n",
    "            return\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            messagebox.showerror(\"Error\", \"Failed to load Image\")\n",
    "            return\n",
    "        self.img_bgr = img\n",
    "        self.show_preview(img)\n",
    "    \n",
    "    def show_preview(self, img_bgr, boxes = None):\n",
    "        img = img_bgr.copy()\n",
    "        if boxes:\n",
    "            for (x1, y1, x2, y2, conf) in boxes:\n",
    "                cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "        h,w = img.shape[:2]\n",
    "        scale = min(600/w, 520/h)\n",
    "        img = cv2.resize(img, (int(w*scale), int(h*scale)))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        pil = Image.fromarray(img_rgb)\n",
    "        self.imgtk = ImageTk.PhotoImage(pil)\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.canvas.create_image(300,260, image= self.imgtk)\n",
    "        \n",
    "    def run_inference(self):\n",
    "        if self.img_bgr is None:\n",
    "            messagebox.showinfo(\"Info\", \"Open an image first\")\n",
    "            return\n",
    "        frame = self.img_bgr.copy()\n",
    "        boxes = detect_faces(face_net, frame, conf_threshold=0.6)\n",
    "        if not boxes or len(boxes[0]) < 4:\n",
    "            messagebox.showinfo(\"No face\", \"No face detected\")\n",
    "            return\n",
    "        \n",
    "        normalized_boxes = []\n",
    "\n",
    "    \n",
    "        for b in boxes:\n",
    "            try:\n",
    "                vals = []\n",
    "                for v in b[:5]:\n",
    "                    if isinstance(v, (list, tuple, np.ndarray)):\n",
    "                        arr = np.array(v).flatten()\n",
    "                        vals.append(float(arr[0]))\n",
    "                    else:\n",
    "                        vals.append(float(v))\n",
    "\n",
    "                if len(vals) >= 4:\n",
    "                    if len(vals) == 5:\n",
    "                        x1, y1, x2, y2, conf = vals\n",
    "                    else:\n",
    "                        x1, y1, x2, y2 = vals[:4]\n",
    "                        conf = 1.0\n",
    "\n",
    "    \n",
    "                    if x2 < 100 and y2 < 100:\n",
    "                        x2 = x1 + x2\n",
    "                        y2 = y1 + y2\n",
    "\n",
    "                    x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "                    normalized_boxes.append((x1, y1, x2, y2, conf))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"⚠️ Skipping box due to error:\", e, \" | Raw box:\", b)\n",
    "\n",
    "\n",
    "\n",
    "        if not normalized_boxes:\n",
    "            messagebox.showinfo(\"No face\", \"No valid face detected\")\n",
    "            return\n",
    "\n",
    "    \n",
    "        boxes = sorted(normalized_boxes, key=lambda b: (b[2]-b[0])*(b[3]-b[1]), reverse=True)\n",
    "        x1, y1, x2, y2, conf = boxes[0]\n",
    "        face = frame[y1:y2, x1:x2].copy()\n",
    "\n",
    "    \n",
    "        if face.size == 0:\n",
    "            messagebox.showerror(\"Face crop error\", \"Face crop empty\")\n",
    "            return\n",
    "\n",
    "    \n",
    "        nat_label, nat_conf, _ = predict_nationality(face)\n",
    "        emo_label, emo_conf, _ = predict_emotion(face)\n",
    "        age_pred = predict_age(face,age_net)\n",
    "        gender_pred= predict_gender(face,gender_net)\n",
    "        dress_colour = estimate_dress_colour(frame, (x1,y1,x2,y2))\n",
    "\n",
    "    \n",
    "        text_lines = []\n",
    "        text_lines.append(f\"Nationality: {nat_label}  ({nat_conf*100:.1f}%)\")\n",
    "        text_lines.append(f\"Emotion:     {emo_label}  ({emo_conf*100:.1f}%)\")\n",
    "        text_lines.append(f\"Gender:      {gender_pred}\")\n",
    "        if nat_label == \"Indian\":\n",
    "            text_lines.append(f\"Age:         {age_pred}\")\n",
    "            text_lines.append(f\"Dress color: {dress_colour}\")\n",
    "        elif nat_label == \"USA\":\n",
    "            text_lines.append(f\"Age:         {age_pred}\")\n",
    "        elif nat_label == \"African\":\n",
    "            text_lines.append(f\"Dress color: {dress_colour}\")\n",
    "    \n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.output.insert(tk.END, \"\\n\".join(text_lines))\n",
    "\n",
    "    \n",
    "        self.show_preview(self.img_bgr, boxes=[boxes[0]])\n",
    "\n",
    "    def clear(self):\n",
    "        self.img_bgr = None\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = App(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f561d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
